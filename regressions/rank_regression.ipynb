{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('refpages', 'backlinks'), ('valid_pages', 'pages'), ('text', 'backlinks'), ('text', 'refpages'), ('image', 'backlinks'), ('image', 'refpages'), ('image', 'text'), ('nofollow', 'backlinks'), ('nofollow', 'refpages'), ('nofollow', 'text'), ('nofollow', 'image'), ('dofollow', 'backlinks'), ('dofollow', 'refpages'), ('dofollow', 'text'), ('dofollow', 'image'), ('dofollow', 'nofollow'), ('gov', 'backlinks'), ('gov', 'refpages'), ('gov', 'text'), ('gov', 'image'), ('gov', 'nofollow'), ('gov', 'dofollow'), ('edu', 'backlinks'), ('edu', 'refpages'), ('edu', 'text'), ('edu', 'image'), ('edu', 'nofollow'), ('edu', 'dofollow'), ('edu', 'gov'), ('html_pages', 'pages'), ('html_pages', 'valid_pages'), ('refclass_c', 'refdomains'), ('refips', 'refdomains'), ('refips', 'refclass_c')]\n",
      "['refclass_c', 'dofollow', 'nofollow', 'valid_pages', 'text', 'gov', 'image', 'refpages', 'refdomains', 'pages']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "log = True\n",
    "traffic_df = pd.read_csv ('data/traffic.csv')\n",
    "attribute_df = pd.read_csv ('data/filtered_attrs.csv')\n",
    "\n",
    "urls_to_remove = ['youtube.com', 'facebook.com']\n",
    "for url in urls_to_remove:\n",
    "    traffic_df = traffic_df[~traffic_df['url'].str.contains(url)]\n",
    "    attribute_df = attribute_df[~attribute_df['url'].str.contains(url)]\n",
    "\n",
    "# merge traffic data with backlinks data\n",
    "url_df = attribute_df.copy() #pd.merge(traffic_df, attribute_df, on='url', how='inner')\n",
    "url_df.dropna(inplace=True)\n",
    "\n",
    "features_to_keep = ['backlinks']#, 'ref_pages''edu', 'gov', 'ugc']\n",
    "drop_vars = ['source', 'url', 'linked_root_domains']\n",
    "url_df.drop(columns=drop_vars, inplace=True)\n",
    "\n",
    "# remove correlated features\n",
    "correlation_matrix = url_df.corr()\n",
    "correlated_features = set()\n",
    "correlated_pairs = []\n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            correlated_features.add(colname)\n",
    "            correlated_pairs.append((correlation_matrix.columns[i], colname))\n",
    "print(correlated_pairs)\n",
    "\n",
    "correlated_features = [x for x in correlated_features if not x in features_to_keep]\n",
    "print(correlated_features)\n",
    "uncorrelated_df = url_df.drop(columns=list(correlated_features))\n",
    "\n",
    "# log of features\n",
    "uncorrelated_log_df = uncorrelated_df.clip(lower=0)\n",
    "\n",
    "if log:\n",
    "    for col in (uncorrelated_log_df.columns):\n",
    "        if col == 'label':\n",
    "            continue\n",
    "        uncorrelated_log_df[col] = np.log(1+uncorrelated_log_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                   rank   R-squared (uncentered):                   0.968\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.968\n",
      "Method:                 Least Squares   F-statistic:                          1.556e+04\n",
      "Date:                Fri, 30 Jun 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        16:51:10   Log-Likelihood:                         -3998.6\n",
      "No. Observations:                4158   AIC:                                      8013.\n",
      "Df Residuals:                    4150   BIC:                                      8064.\n",
      "Df Model:                           8                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "backlinks          0.3088      0.004     68.662      0.000       0.300       0.318\n",
      "ugc               -0.0147      0.008     -1.783      0.075      -0.031       0.001\n",
      "redirect          -0.0489      0.007     -6.704      0.000      -0.063      -0.035\n",
      "canonical         -0.0372      0.004     -9.775      0.000      -0.045      -0.030\n",
      "edu               -0.0451      0.006     -7.905      0.000      -0.056      -0.034\n",
      "html_pages        -0.0878      0.011     -8.192      0.000      -0.109      -0.067\n",
      "links_internal     0.0865      0.009      9.991      0.000       0.070       0.103\n",
      "links_external     0.0174      0.008      2.229      0.026       0.002       0.033\n",
      "==============================================================================\n",
      "Omnibus:                      406.312   Durbin-Watson:                   1.950\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              795.957\n",
      "Skew:                          -0.643   Prob(JB):                    1.45e-173\n",
      "Kurtosis:                       4.715   Cond. No.                         33.8\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "insignificant_features = { \n",
    "    'label': [],#'pages', 'backlinks', 'valid_pages', 'canonical', 'gov','edu', 'rss', 'alternate', 'html_pages', 'links_internal', 'links_external'],\n",
    "    'rank': ['rss','alternate'],#'canonical', 'rss', 'alternate', 'html_pages', 'links_external', 'linked_root_domains'],\n",
    "    'traffic': ['rss', 'sponsored', 'redirect', 'ugc','canonical','edu','alternate', 'label', 'refips', 'links_internal'],#'linked_root_domains', 'links_internal', 'refpages'],\n",
    "    'traffic_top3': ['rss', 'linked_root_domains', 'links_internal', 'html_pages'],\n",
    "    'traffic_top10': [],\n",
    "    'cost':[],\n",
    "    'positions':[],\n",
    "}\n",
    "reg_var = 'rank'\n",
    "\n",
    "# train, test, labels, y_test = train_test_split(uncorrelated_log_df, traffic_df, test_size=0.1, random_state=63)\n",
    "train = uncorrelated_log_df.copy()\n",
    "test = uncorrelated_log_df.copy()\n",
    "labels = traffic_df.copy()\n",
    "y_test = traffic_df.copy()\n",
    "y_train = np.log(1+labels[reg_var]) if log else labels[reg_var]\n",
    "# y_train = labels[reg_var]\n",
    "X_train = train.drop(columns=insignificant_features[reg_var]) \n",
    "\n",
    "features = X_train.columns.to_list()\n",
    "\n",
    "# regressor = LinearRegression()  \n",
    "# model = regressor.fit(X_train, y_train)\n",
    "# print(\"Coef:\", model.coef_)\n",
    "# print(\"Constant:\", model.intercept_)\n",
    "# print(\"R2:\", model.score(X_train, y_train))\n",
    "model = sm.OLS\n",
    "est = sm.OLS(y_train, X_train.drop(columns=['label', 'sponsored', 'refips']))\n",
    "est2 = est.fit()\n",
    "\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = est2._results.summary()\n",
    "f = open(reg_var + \"_regression_base.txt\", \"w\")\n",
    "f.write(res.as_latex())\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by reliability label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                   rank   R-squared (uncentered):                   0.937\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.936\n",
      "Method:                 Least Squares   F-statistic:                              2300.\n",
      "Date:                Fri, 30 Jun 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        16:51:10   Log-Likelihood:                         -1477.0\n",
      "No. Observations:                1248   AIC:                                      2970.\n",
      "Df Residuals:                    1240   BIC:                                      3011.\n",
      "Df Model:                           8                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "backlinks          0.2905      0.008     34.243      0.000       0.274       0.307\n",
      "ugc                0.0462      0.017      2.688      0.007       0.012       0.080\n",
      "redirect          -0.0509      0.016     -3.111      0.002      -0.083      -0.019\n",
      "canonical         -0.0606      0.013     -4.642      0.000      -0.086      -0.035\n",
      "edu               -0.0415      0.015     -2.729      0.006      -0.071      -0.012\n",
      "html_pages        -0.0212      0.024     -0.892      0.373      -0.068       0.025\n",
      "links_internal     0.0601      0.017      3.546      0.000       0.027       0.093\n",
      "links_external    -0.0182      0.014     -1.298      0.195      -0.046       0.009\n",
      "==============================================================================\n",
      "Omnibus:                       41.775   Durbin-Watson:                   1.993\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               46.623\n",
      "Skew:                          -0.420   Prob(JB):                     7.52e-11\n",
      "Kurtosis:                       3.436   Cond. No.                         23.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13687/2318891899.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reliable.drop(columns=['label', 'refips', 'sponsored'] + insignificant_features[reg_var], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# fit model on reliable data\n",
    "X_train_reliable = train[(train['label'] < 4)]\n",
    "y_train_reliable = np.log(1+labels[reg_var][(train['label'] < 4)])\n",
    "X_train_reliable.drop(columns=['label', 'refips', 'sponsored'] + insignificant_features[reg_var], inplace=True)\n",
    "# X_train_reliable = X_train_reliable[features_to_keep]\n",
    "est_reliable = sm.OLS(y_train_reliable, X_train_reliable)\n",
    "est2_reliable = est_reliable.fit()\n",
    "print(est2_reliable.summary())\n",
    "\n",
    "res = est2_reliable._results.summary()\n",
    "f = open(\"traffic_reg/\"+reg_var+\"_lable_regression_base_unrel.csv\", \"w\")\n",
    "f.write(res.as_csv())\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                   rank   R-squared (uncentered):                   0.959\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.959\n",
      "Method:                 Least Squares   F-statistic:                          3.281e+04\n",
      "Date:                Fri, 30 Jun 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        16:59:10   Log-Likelihood:                         -4471.0\n",
      "No. Observations:                4158   AIC:                                      8948.\n",
      "Df Residuals:                    4155   BIC:                                      8967.\n",
      "Df Model:                           3                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "backlinks          0.2715      0.003     82.945      0.000       0.265       0.278\n",
      "links_internal     0.0313      0.007      4.170      0.000       0.017       0.046\n",
      "links_external    -0.0281      0.008     -3.512      0.000      -0.044      -0.012\n",
      "==============================================================================\n",
      "Omnibus:                      160.800   Durbin-Watson:                   1.895\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              215.588\n",
      "Skew:                          -0.401   Prob(JB):                     1.53e-47\n",
      "Kurtosis:                       3.776   Cond. No.                         20.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "keep_only_intervention_features = True\n",
    "\n",
    "if keep_only_intervention_features:\n",
    "    test_features_to_keep = ['backlinks', 'links_internal', 'links_external']#, 'refpages']\n",
    "else:\n",
    "    test_features_to_keep = X_train.columns.to_list()\n",
    "    test_features_to_keep.remove('label')\n",
    "\n",
    "X_train_test = X_train[test_features_to_keep]\n",
    "est_clean = sm.OLS(y_train, X_train_test)\n",
    "est_clean_2 = est_clean.fit()\n",
    "print(est_clean_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff(res_df, pre_col, post_col):\n",
    "# clip negative values\n",
    "    res_df[post_col] = res_df[post_col].clip(lower=0)\n",
    "    # compute difference between t_clean and t_inter\n",
    "    try:\n",
    "        if log:\n",
    "            res_df['diff'] = np.exp(res_df[post_col]) / np.exp(res_df[pre_col])\n",
    "        else:\n",
    "            res_df['diff'] = (res_df[post_col]) / (res_df[pre_col])\n",
    "    except:\n",
    "        res_df['diff'] = 1\n",
    "    # group test_res by label\n",
    "    return res_df.groupby('label').mean()['diff']\n",
    "\n",
    "\n",
    "def pre_post_intervention_diff(path):\n",
    "    weighted_df = pd.read_csv(path)[['url', 'pre_backlinks', 'pre_refpages', 'post_backlinks', 'post_refpages']]\n",
    "    weighted_df['pre_backlinks'] = np.log(weighted_df['pre_backlinks']+1)\n",
    "    weighted_df['post_backlinks'] = np.log(weighted_df['post_backlinks']+1)\n",
    "\n",
    "    uncorrelated_log_df['url'] = attribute_df['url']\n",
    "    weighted_df = pd.merge(weighted_df, uncorrelated_log_df, on='url', how='inner')\n",
    "\n",
    "    link_relevancy_weighting_regression_df = weighted_df.copy()\n",
    "    link_relevancy_weighting_regression_df['label'].replace({1:3, 6:5}, inplace=True)\n",
    "    link_relevancy_weighting_regression_df = link_relevancy_weighting_regression_df[['url', 'label']]\n",
    "    clean_attrs = list(test_clean.columns[1:])\n",
    "    link_relevancy_weighting_regression_df['t_clean'] = est_clean_2.predict(weighted_df[['pre_backlinks'] + clean_attrs])\n",
    "    link_relevancy_weighting_regression_df['t_inter'] = est_clean_2.predict(weighted_df[['post_backlinks'] + clean_attrs])\n",
    "    return compute_diff(link_relevancy_weighting_regression_df, 't_clean', 't_inter')\n",
    "\n",
    "test_clean = test.drop(columns=['label'])\n",
    "test_clean = test_clean[test_features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/dev/envs/env_seo/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negated_sample</td>\n",
       "      <td>0.588435</td>\n",
       "      <td>0.703959</td>\n",
       "      <td>0.834559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>0.868463</td>\n",
       "      <td>0.855223</td>\n",
       "      <td>0.861729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighted_max_sample</td>\n",
       "      <td>0.927078</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.942356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_max</td>\n",
       "      <td>0.966209</td>\n",
       "      <td>0.964591</td>\n",
       "      <td>0.975491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combined_sampled</td>\n",
       "      <td>0.502415</td>\n",
       "      <td>0.586628</td>\n",
       "      <td>0.732273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>combined</td>\n",
       "      <td>0.673450</td>\n",
       "      <td>0.771822</td>\n",
       "      <td>0.876109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negated_0.2</td>\n",
       "      <td>0.915206</td>\n",
       "      <td>0.940386</td>\n",
       "      <td>0.971270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name         3         4         5\n",
       "0       negated_sample  0.588435  0.703959  0.834559\n",
       "1        weighted_mean  0.868463  0.855223  0.861729\n",
       "2  weighted_max_sample  0.927078  0.916969  0.942356\n",
       "3         weighted_max  0.966209  0.964591  0.975491\n",
       "4     combined_sampled  0.502415  0.586628  0.732273\n",
       "5             combined  0.673450  0.771822  0.876109\n",
       "6          negated_0.2  0.915206  0.940386  0.971270"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(columns=['name',3,4,5])\n",
    "\n",
    "experiments = {\n",
    "    'negated_sample': 'results/final/link_scheme_negated_only.csv',\n",
    "    'weighted_mean': 'results/final/backlink_relevancy_weighted_attributes_sampled_mean.csv',\n",
    "    'weighted_max_sample': 'results/final/backlink_relevancy_weighted_attributes_sampled_max.csv',\n",
    "    'weighted_max': 'results/final/backlink_relevancy_weighted_attributes_not_sampled_max.csv',\n",
    "    'combined_sampled': 'results/final/backlink_relevancy_weighted_negated_combined.csv',\n",
    "    'combined': 'results/final/backlink_relevancy_weighted_negated_combined_not_sampled.csv',\n",
    "    # 'negated': 'results/final/link_scheme_negated_attributes.csv',\n",
    "    'negated_0.2': 'results/final/link_scheme_negated_attributes_sampled.csv'\n",
    "}\n",
    "\n",
    "for name, path in experiments.items():\n",
    "    res = pre_post_intervention_diff(path)\n",
    "    res_df.loc[len(res_df)] = [name]+ res.values.tolist()\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/dev/envs/env_seo/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negated_sample</td>\n",
       "      <td>0.588435</td>\n",
       "      <td>0.703959</td>\n",
       "      <td>0.834559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>0.868463</td>\n",
       "      <td>0.855223</td>\n",
       "      <td>0.861729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighted_max_sample</td>\n",
       "      <td>0.927078</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.942356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_max</td>\n",
       "      <td>0.966209</td>\n",
       "      <td>0.964591</td>\n",
       "      <td>0.975491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combined_sampled</td>\n",
       "      <td>0.502415</td>\n",
       "      <td>0.586628</td>\n",
       "      <td>0.732273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>combined</td>\n",
       "      <td>0.673450</td>\n",
       "      <td>0.771822</td>\n",
       "      <td>0.876109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negated_0.2</td>\n",
       "      <td>0.915206</td>\n",
       "      <td>0.940386</td>\n",
       "      <td>0.971270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>control_0</td>\n",
       "      <td>0.144644</td>\n",
       "      <td>0.131093</td>\n",
       "      <td>0.028625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>control_0.5</td>\n",
       "      <td>0.829917</td>\n",
       "      <td>0.828454</td>\n",
       "      <td>0.828537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>control_1</td>\n",
       "      <td>1.000088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name         3         4         5\n",
       "0       negated_sample  0.588435  0.703959  0.834559\n",
       "1        weighted_mean  0.868463  0.855223  0.861729\n",
       "2  weighted_max_sample  0.927078  0.916969  0.942356\n",
       "3         weighted_max  0.966209  0.964591  0.975491\n",
       "4     combined_sampled  0.502415  0.586628  0.732273\n",
       "5             combined  0.673450  0.771822  0.876109\n",
       "6          negated_0.2  0.915206  0.940386  0.971270\n",
       "7            control_0  0.144644  0.131093  0.028625\n",
       "8          control_0.5  0.829917  0.828454  0.828537\n",
       "9            control_1  1.000088  1.000000  1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for control in [0, 0.5, 1]:\n",
    "    test_intervention = test_clean.copy()\n",
    "    test_res = test_intervention.copy()\n",
    "    test_intervention['backlinks'] = np.log(np.exp(test_intervention['backlinks']) * control)\n",
    "    # test_intervention['refpages'] = test_intervention['refpages'] * 0\n",
    "\n",
    "    test_res['t_clean'] = est_clean_2.predict(test_clean)\n",
    "    test_res['t_inter'] = est_clean_2.predict(test_intervention)\n",
    "    test_res['label'] = test['label']\n",
    "\n",
    "    test_res['label'].replace({1:3, 6:5}, inplace=True)\n",
    "\n",
    "    res = compute_diff(test_res, 't_clean', 't_inter')\n",
    "    res_df.loc[len(res_df)] = ['control_' + str(control)]+ res.values.tolist()\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>name</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.673450</td>\n",
       "      <td>0.771822</td>\n",
       "      <td>0.876109</td>\n",
       "      <td>Combined</td>\n",
       "      <td>0.306946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.915206</td>\n",
       "      <td>0.940386</td>\n",
       "      <td>0.971270</td>\n",
       "      <td>Link Scheme Removal</td>\n",
       "      <td>0.086948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.966209</td>\n",
       "      <td>0.964591</td>\n",
       "      <td>0.975491</td>\n",
       "      <td>Relevancy Weighted (max)</td>\n",
       "      <td>0.020182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.502415</td>\n",
       "      <td>0.586628</td>\n",
       "      <td>0.732273</td>\n",
       "      <td>Combined*</td>\n",
       "      <td>0.375502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.588435</td>\n",
       "      <td>0.703959</td>\n",
       "      <td>0.834559</td>\n",
       "      <td>Link Scheme Removal*</td>\n",
       "      <td>0.376725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.927078</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.942356</td>\n",
       "      <td>Relevancy Weighted (max)*</td>\n",
       "      <td>0.040666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.868463</td>\n",
       "      <td>0.855223</td>\n",
       "      <td>0.861729</td>\n",
       "      <td>Relevancy Weighted (mean)*</td>\n",
       "      <td>-0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.144644</td>\n",
       "      <td>0.131093</td>\n",
       "      <td>0.028625</td>\n",
       "      <td>Control 100\\%</td>\n",
       "      <td>-0.218488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.829917</td>\n",
       "      <td>0.828454</td>\n",
       "      <td>0.828537</td>\n",
       "      <td>Control 50\\%</td>\n",
       "      <td>-0.001298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Control 0\\%</td>\n",
       "      <td>-0.000088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          3         4         5                        name    metric\n",
       "0  0.673450  0.771822  0.876109                    Combined  0.306946\n",
       "1  0.915206  0.940386  0.971270         Link Scheme Removal  0.086948\n",
       "2  0.966209  0.964591  0.975491    Relevancy Weighted (max)  0.020182\n",
       "3  0.502415  0.586628  0.732273                   Combined*  0.375502\n",
       "4  0.588435  0.703959  0.834559        Link Scheme Removal*  0.376725\n",
       "5  0.927078  0.916969  0.942356   Relevancy Weighted (max)*  0.040666\n",
       "6  0.868463  0.855223  0.861729  Relevancy Weighted (mean)* -0.000227\n",
       "7  0.144644  0.131093  0.028625               Control 100\\% -0.218488\n",
       "8  0.829917  0.828454  0.828537                Control 50\\% -0.001298\n",
       "9  1.000088  1.000000  1.000000                 Control 0\\% -0.000088"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_order = [\"combined\", \"negated_0.2\", \"weighted_max\", \"combined_sampled\", \"negated_sample\", \"weighted_max_sample\", \"weighted_mean\", \"control_0\", \"control_0.5\", \"control_1\"]\n",
    "res_df.set_index('name', inplace=True)\n",
    "res_df = res_df.loc[col_order]\n",
    "res_df['name'] = ['Combined', 'Link Scheme Removal', 'Relevancy Weighted (max)', 'Combined*', 'Link Scheme Removal*', 'Relevancy Weighted (max)*', 'Relevancy Weighted (mean)*', 'Control 100\\%', 'Control 50\\%', 'Control 0\\%']\n",
    "res_df.reset_index(drop=True, inplace=True)\n",
    "res_df['metric'] = (1 - res_df[4]) + (1 - res_df[3]) - (1 - res_df[5]) * 2\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('results/final/regression_'+reg_var+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env_seo': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7849e7bbfc8a172a055556edae1fedc2a22a02afeeefc5ab99ca2ca8666466e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
